{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3eCevDEq5GdK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "XP6NUCVr6eoN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/myDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QodS1RL56irl",
        "outputId": "7cfd9b59-5ef2-462d-d678-b4b26fdb75ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/myDrive; to attempt to forcibly remount, call drive.mount(\"/content/myDrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vl6DlQV5ROd",
        "outputId": "881ba08f-7cb8-40d2-a528-21eee532f0e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data shape (20800, 4)\n",
            "                                                title              author  \\\n",
            "id                                                                          \n",
            "0   House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
            "1   FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
            "2                   Why the Truth Might Get You Fired  Consortiumnews.com   \n",
            "3   15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
            "4   Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
            "\n",
            "                                                 text  label  \n",
            "id                                                            \n",
            "0   House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
            "1   Ever get the feeling your life circles the rou...      0  \n",
            "2   Why the Truth Might Get You Fired October 29, ...      1  \n",
            "3   Videos 15 Civilians Killed In Single US Airstr...      1  \n",
            "4   Print \\nAn Iranian woman has been sentenced to...      1  \n",
            "test data shape (5200, 4)\n",
            "      id                                              title  \\\n",
            "0  20800  Specter of Trump Loosens Tongues, if Not Purse...   \n",
            "1  20801  Russian warships ready to strike terrorists ne...   \n",
            "2  20802  #NoDAPL: Native American Leaders Vow to Stay A...   \n",
            "3  20803  Tim Tebow Will Attempt Another Comeback, This ...   \n",
            "4  20804                    Keiser Report: Meme Wars (E995)   \n",
            "\n",
            "                    author                                               text  \n",
            "0         David Streitfeld  PALO ALTO, Calif.  —   After years of scorning...  \n",
            "1                      NaN  Russian warships ready to strike terrorists ne...  \n",
            "2            Common Dreams  Videos #NoDAPL: Native American Leaders Vow to...  \n",
            "3            Daniel Victor  If at first you don’t succeed, try a different...  \n",
            "4  Truth Broadcast Network  42 mins ago 1 Views 0 Comments 0 Likes 'For th...  \n"
          ]
        }
      ],
      "source": [
        "train = pd.read_csv('/content/myDrive/MyDrive/fake-news/train.csv')\n",
        "test = pd.read_csv('/content/myDrive/MyDrive/fake-news/test.csv')\n",
        "train_data = train.copy()\n",
        "test_data = test.copy()\n",
        "\n",
        "train_data = train_data.set_index('id', drop = True)\n",
        "\n",
        "print('train data shape', train_data.shape)\n",
        "print(train_data.head())\n",
        "\n",
        "print('test data shape', test_data.shape)\n",
        "print(test_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "k7YLALqD5gOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c976242-b019-4f7b-cc4f-cec26d66fc9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "missing values counts\n",
            " title      558\n",
            "author    1957\n",
            "text        39\n",
            "label        0\n",
            "dtype: int64\n",
            "missing values counts n title     0\n",
            "author    0\n",
            "text      0\n",
            "label     0\n",
            "dtype: int64\n",
            "train data length\n",
            "                                                 title              author  \\\n",
            "id                                                                          \n",
            "0   House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
            "1   FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
            "2                   Why the Truth Might Get You Fired  Consortiumnews.com   \n",
            "3   15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
            "4   Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
            "\n",
            "                                                 text  label  length  \n",
            "id                                                                    \n",
            "0   House Dem Aide: We Didn’t Even See Comey’s Let...      1    4930  \n",
            "1   Ever get the feeling your life circles the rou...      0    4160  \n",
            "2   Why the Truth Might Get You Fired October 29, ...      1    7692  \n",
            "3   Videos 15 Civilians Killed In Single US Airstr...      1    3237  \n",
            "4   Print \\nAn Iranian woman has been sentenced to...      1     938  \n",
            "min data length 1 , max data length 142961 , average data length 4553\n",
            "count of less then 50 character 207\n",
            "min data length 50 , max data length 142961 , average data length 4598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-a8d1e3be196c>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['length'] = length\n"
          ]
        }
      ],
      "source": [
        "print('missing values counts\\n', train_data.isnull().sum())\n",
        "\n",
        "# dropping missing values from text columns alone.\n",
        "train_data[['title', 'author']] = train_data[['title', 'author']].fillna(value = 'Missing')\n",
        "train_data = train_data.dropna()\n",
        "print('missing values counts n', train_data.isnull().sum())\n",
        "\n",
        "length = []\n",
        "[length.append(len(str(text))) for text in train_data['text']]\n",
        "train_data['length'] = length\n",
        "print('train data length\\n', train_data.head())\n",
        "\n",
        "print('min data length', min(train_data['length']), ', max data length', max(train_data['length']), ', average data length', round(sum(train_data['length'])/len(train_data['length'])))\n",
        "\n",
        "print('count of less then 50 character', len(train_data[train_data['length'] < 50]))\n",
        "\n",
        "# dropping the outliers\n",
        "train_data = train_data.drop(train_data['text'][train_data['length'] < 50].index, axis = 0)\n",
        "print('min data length', min(train_data['length']), ', max data length', max(train_data['length']), ', average data length', round(sum(train_data['length'])/len(train_data['length'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gkRCID7b5m4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c2c171b-e2e2-42fe-9c7a-3689b48cd599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape (20554, 4500)\n",
            "Y shape (20554,)\n"
          ]
        }
      ],
      "source": [
        "max_features = 4500\n",
        "\n",
        "# Tokenizing the text - converting the words, letters into counts or numbers.\n",
        "# We dont need to explicitly remove the punctuations. we have an inbuilt option in Tokenizer for this purpose\n",
        "tokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "tokenizer.fit_on_texts(texts = train_data['text'])\n",
        "X = tokenizer.texts_to_sequences(texts = train_data['text'])\n",
        "\n",
        "# now applying padding to make them even shaped.\n",
        "X = pad_sequences(sequences = X, maxlen = max_features, padding = 'pre')\n",
        "\n",
        "print('X shape', X.shape)\n",
        "y = train_data['label'].values\n",
        "print('Y shape', y.shape)\n",
        "\n",
        "# splitting the data training data for training and validation.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hP987O9_5w92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb76b426-21b5-4899-8754-e47e36412207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m597s\u001b[0m 9s/step - accuracy: 0.7365 - loss: 0.5494\n",
            "Epoch 2/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 9s/step - accuracy: 0.9247 - loss: 0.2182\n",
            "Epoch 3/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 9s/step - accuracy: 0.9410 - loss: 0.1681\n",
            "Epoch 4/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 9s/step - accuracy: 0.9437 - loss: 0.1562\n",
            "Epoch 5/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 9s/step - accuracy: 0.9562 - loss: 0.1289\n",
            "Epoch 6/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m592s\u001b[0m 9s/step - accuracy: 0.9647 - loss: 0.1038\n",
            "Epoch 7/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 9s/step - accuracy: 0.9576 - loss: 0.1245\n",
            "Epoch 8/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 9s/step - accuracy: 0.9619 - loss: 0.1133\n",
            "Epoch 9/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m580s\u001b[0m 9s/step - accuracy: 0.9632 - loss: 0.1013\n",
            "Epoch 10/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 9s/step - accuracy: 0.9650 - loss: 0.1012\n"
          ]
        }
      ],
      "source": [
        "# LSTM Neural Network\n",
        "lstm_model = Sequential(name = 'lstm_nn_model')\n",
        "lstm_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
        "lstm_model.add(layer = LSTM(units = 120, dropout = 0.2, recurrent_dropout = 0.2, name = '2nd_layer'))\n",
        "lstm_model.add(layer = Dropout(rate = 0.5, name = '3rd_layer'))\n",
        "lstm_model.add(layer = Dense(units = 120,  activation = 'relu', name = '4th_layer'))\n",
        "lstm_model.add(layer = Dropout(rate = 0.5, name = '5th_layer'))\n",
        "lstm_model.add(layer = Dense(units = len(set(y)),  activation = 'sigmoid', name = 'output_layer'))\n",
        "# compiling the model\n",
        "lstm_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "lstm_model_fit = lstm_model.fit(X_train, y_train, epochs = 10, batch_size = 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "W1PqsgSV58F0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e987d2-19f6-4cd8-c8b3-fa2402dbc668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_data shape (5200, 4)\n",
            "test_data shape (5200, 3)\n",
            "test_data shape (5200, 3)\n",
            "title     0\n",
            "author    0\n",
            "text      0\n",
            "dtype: int64\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 2s/step\n",
            "lstm_prediction [0 0 1 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "test_data = test.copy()\n",
        "print('test_data shape', test_data.shape)\n",
        "\n",
        "test_data = test_data.set_index('id', drop = True)\n",
        "print('test_data shape', test_data.shape)\n",
        "\n",
        "test_data = test_data.fillna(' ')\n",
        "print('test_data shape', test_data.shape)\n",
        "print(test_data.isnull().sum())\n",
        "\n",
        "tokenizer.fit_on_texts(texts = test_data['text'])\n",
        "test_text = tokenizer.texts_to_sequences(texts = test_data['text'])\n",
        "\n",
        "test_text = pad_sequences(sequences = test_text, maxlen = max_features, padding = 'pre')\n",
        "\n",
        "lstm_prediction = lstm_model.predict(test_text)\n",
        "\n",
        "lstm_prediction_vec = np.argmax(lstm_prediction, axis=1)\n",
        "\n",
        "print(\"lstm_prediction\", lstm_prediction_vec)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "real_results = pd.read_csv('/content/myDrive/MyDrive/fake-news/submit.csv')\n",
        "real_results = real_results[\"label\"]\n",
        "\n",
        "print(\"test_results\", real_results)\n",
        "print(real_results.shape)\n",
        "\n",
        "accuracy = accuracy_score(real_results, lstm_prediction_vec)\n",
        "precision = precision_score(real_results, lstm_prediction_vec, average='weighted')\n",
        "recall = recall_score(real_results, lstm_prediction_vec, average='weighted')\n",
        "f1 = f1_score(real_results, lstm_prediction_vec, average='weighted')\n",
        "\n",
        "classification_rep = classification_report(real_results, lstm_prediction_vec)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)"
      ],
      "metadata": {
        "id": "U50y57zoI8kM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48353953-0ea8-4188-94c0-30aa81fb44c4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_results 0       0\n",
            "1       1\n",
            "2       0\n",
            "3       1\n",
            "4       1\n",
            "       ..\n",
            "5195    0\n",
            "5196    1\n",
            "5197    0\n",
            "5198    1\n",
            "5199    0\n",
            "Name: label, Length: 5200, dtype: int64\n",
            "(5200,)\n",
            "Accuracy: 0.61\n",
            "Precision: 0.64\n",
            "Recall: 0.61\n",
            "F1-Score: 0.60\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.76      0.63      2339\n",
            "           1       0.71      0.49      0.58      2861\n",
            "\n",
            "    accuracy                           0.61      5200\n",
            "   macro avg       0.63      0.62      0.61      5200\n",
            "weighted avg       0.64      0.61      0.60      5200\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}